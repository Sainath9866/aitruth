Detecting Inaccuracies in AI-Generated Educational Content: A Study on ChatGPT Responses - Is ChatGPT a reliable for Students 




 






 



Abstract- Students enjoy using AI tools such as ChatGPT, Gemini, Grok, DeepSeek, and Copilot because they are always easy to access. They serve as quick and helpful study partners, much like grabbing a hot cup of coffee on your way to class. These apps provide quick feedback and break long materials into manageable pieces. However, there's still a question about how reliable their answers are, similar to pressing a shiny plastic apple and expecting it to smell like fruit. This study explores how much students depend on computer-generated text for learning. For example, they might skim an AI summary to understand a challenging chapter filled with formulas. We posed a set of carefully crafted questions to several AI systems covering computer science, math, physics, and the humanities, similar to dropping brightly colored marbles from each subject into one jar. Experts reviewed the responses for accuracy, clarity, and completeness, examining each line like a jeweler inspecting a diamond under bright light.The analysis showed that while the AI tools generally provided clear, well-organized answers, they often faltered with math and reasoning. For instance, some tools mixed up numbers in a simple equation like 4 + 5. Certain tools excelled at specific tasks; for example, Copilot performed exceptionally well with coding challenges, while others showed a broader, more balanced understanding of how concepts connect. Overall, the findings indicate that learning with AI can open up new pathways but may also lead to missteps, like noticing a bright shortcut that suddenly vanishes into thick fog. While these technologies create opportunities and spark curiosity, they also highlight weaknesses, such as overlooked facts, diminished critical thinking, and a gradual drift away from human intuition, much like losing the familiar feel of a book's pages between your fingers. This research contributes to the ongoing discussion about responsible AI use in education and encourages students to engage with these tools more thoughtfully, similar to taking a moment to verify a source before accepting it as true.t. Keywords such as artificial intelligence, educational content, AI tools, accuracy, reliability, AI in education, critical thinking, and student learning all shape the rhythm of a lesson-like the soft click of keys just before a new idea sparks to life.
I. INTRODUCTION
Artificial intelligence has entered classrooms and changed how students find, understand, and interact with information. They can search for answers faster than flipping through a textbook. As natural language processing and machine learning advance, more students are turning to AI tools like ChatGPT, Gemini, Grok, DeepSeek, and GitHub Copilot to aid their learning. Many use these tools late at night, with a laptop softly humming in the background. These tools provide clear explanations, neat summaries, and straightforward solutions, making them popular with learners who want quick, straightforward answers. Students turn to AI for instant information and concise explanations, allowing them to learn at their own pace-like having a quiet tutor in the corner of a dim screen. AI offers hands-on, conversational support that feels personal and effective. It resembles chatting with a real helper rather than using old tools.
While these benefits are appealing, they come with significant drawbacks. AI tools can sometimes provide incorrect or incomplete information, and students-especially those new to the subject-may not notice the errors, much like missing a smudge of ink on a crowded page. As a result, they might misunderstand concepts, misinterpret what's happening, or rely too much on technology-trusting the glowing screen more than their own judgment. This study addresses a key issue: AI tools deliver quick answers, but students often overlook the subtle mistakes that distort their understanding, similar to how a minor misprint can change the meaning of a sentence. It also questions whether today's popular AI tools can genuinely create accurate and helpful educational content-a problem as obvious as a glaring typo on the screen.
This study will evaluate how well selected AI tools perform by checking their accuracy, clarity, completeness, and consistency. It will highlight their strengths and weaknesses across different fields, for example, comparing how effectively each handles a complex data analysis task. From there, it will propose practical ideas for how AI could assist in classrooms to make teaching safer and more efficient.
II. LITERATURE SURVEY

   B. Sharimbayev and S. stood next to each other, the smell of fresh ink rising from the papers between them. In 2025, Kadyrov [1] compared machine learning and deep learning models to see which one could better identify AI-generated text. He tested each model on a set of clear, computer-written sentences. The researchers used three models on the COLING Workshop on MGT Detection Task 1 corpus: Logistic Regression with TF-IDF features, a Bi-LSTM network, and a fine-tuned DistilBERT model that ran quietly on their GPUs. The corpus included samples from five AI models and pieces written by humans-the kind that showed traces of ink and coffee. The experiments revealed that Bi-LSTM performed the best with 90.09% accuracy and a 90.02% F1-score, numbers sharp enough to stand out like crisp digits on a screen. It achieved 95.9% accuracy when categorizing texts into two groups-AI-made or human-written-similar to noticing the subtle mechanical rhythm in a machine's phrasing..
   The weathered sign showed only a single "G." Its paint was flaking and dull in the glare of the afternoon sun. In 2025, Erol and the team tested how reliable and valid several tools were for spotting AI-generated text. These systems aimed to tell genuine academic writing apart from work produced by machines typing away in silence. In 2025, the team explored how reliable and precise various tools were at detecting AI-generated text. These systems were designed to distinguish authentic human academic writing from work created by AI, sometimes by noticing a subtle shift in tone or an oddly stiff phrase. Beyond the ethical concerns, this also revealed the technical issues of using them in class. For example, a program might freeze just as a student's hand hovers over the keyboard. Detection accuracy varied greatly. One test was spot on, while another missed badly like a dart gone astray. Because of this, you can't fully trust the results.
    A faint hum filled the air as C brushed a bit of dust from the table. The letter G glowed faintly, like chalk catching the last bit of sunlight. In 2025, Reddy and colleagues proposed a web-based application called AI vs Human Text Detector. This tool identifies AI-generated text and human-written content using an NLP technique. The system uses a pre-trained GPT-2 model to analyze features like perplexity and burstiness. It traces each shift in tone like a flicker of light across a page. It reached 99.2% detection accuracy, surpassing every ERT, CNN, and BERT-CNN model. It was like spotting a faint signal through static before the others could.
    A single red leaf drifted down and landed softly on the wet pavement. Shoaib and his team discussed the growing threats to global information integrity caused by deepfakes and AI-driven misinformation. They explored how deepfakes and AI-powered misinformation are breaking down the world's trust in information, much like watching a photo shimmer into something false right before your eyes. This study examined how large, model-driven generative AI makes fake content appear startlingly real. This technology shapes society, sways politics, and undermines our sense of privacy. With that in mind, they suggested a unified defense setup that combines effective detection algorithms, subtle digital watermarks, and machine-learning authentication, similar to layers of paint protecting a surface from wear.
   In 2024, L. stepped outside into the chill morning air, and breath misted like smoke. I stepped outside into the crisp morning air, and my breath curled like smoke in the pale light. Liu and the team shared their findings after creating EduGuard‑LLM, a model to spot AI‑written student work. Their notes lay scattered on a desk that had a faint smell of coffee. This small scene reflected the growing concerns about academic integrity that moved through classrooms like pencils on paper.
   A faint hum filled the air as L tapped a pen against the desk, lost in thought. Peeling an orange releases a burst of fresh citrus that wakes you right up. In 2024, Kalra and the team ran an experiment to spot AI-written text, using two complementary methods-a classification-based SVM model and another built on statistical linguistic analysis, sharp as ink on fresh paper. In 2024, [6] ran an experiment to detect AI‑generated text, using two approaches: a classification SVM model and a statistical linguistic analysis that traced patterns as steady and exact as a heartbeat's pulse. Their system hit an impressive 99.87% accuracy on raw data, but the other two models slipped to 87.58% and 84.91% when reading rephrased text-almost like trying to make out a street sign through steady rain.
   Raja Subramanian and his team, with chalk dust on their hands from the lab boards, continued their work. In 2025, they proposed a real-time plagiarism detection system that used three transformer-based models: multi-qa-mpnet-base-dot-v1, roberta-large-openai-detector, and xlm-roberta-large. This system operated smoothly, like gears in a well-oiled machine. It checks how well words fit their context, not just how similar they look, to identify semantic plagiarism, AI-generated text, or writing in different languages. The system now detects plagiarism with about 95% accuracy. It also runs faster and more reliably than anything we've used before, spotting copied lines almost as soon as they appear on screen.
   A faint scent of coffee drifted through the air as C looked up and smiled. Mohammed and S. stood close together, and the evening air felt warm against their backs. In 2024, Rocke [8] examined how free-to-use generative AI posed a growing threat to the integrity of academic assessments, as if someone had let a clever ghost slip into the exam room. This ongoing study explored research on how educators use post-assessment strategies to detect unauthorized AI in student work, like spotting oddly precise phrasing that doesn't match a student's usual tone. After reviewing studies from ERIC and IEEE Xplore, researchers identified 11 tools-five general ones and six designed specifically for AI-that together show how traditional and modern methods now combine to verify who really wrote the work. The study emphasized that even though detection tools are already in place, integrity frameworks must continue to evolve to keep up with the fast-changing pace of AI, like trying to hold water in your hands as it flows.
   In 2023, Elkhatat and his colleagues examined how well detection tools could tell if a piece of writing came from a human or an AI generator. This is similar to noticing the faint unevenness of ink on paper. They explored how accurately these tools could differentiate between human writing and AI-generated text. This is like observing the uneven tap of keys when typing by hand, compared to the smooth, steady flow of a machine. We tested samples from ChatGPT-3.5, ChatGPT-4, and human writers using five detectors: OpenAI, Writer, Copyleaks, GPTZero, and CrossPlag. The results flickered across the monitor. The detectors performed well with GPT-3.5 outputs, but struggled with GPT-4 and human writing, sounding false alarms like a smoke detector with low batteries. L also discusses the growing concerns about academic honesty in this new era of AI-generated work, particularly with tools like ChatGPT. 
In 2023, Uzun [10] worked while the air carried a light scent of rain and a feeling of a fresh start. The study examined various detection tools, including Copyleaks, Turnitin, metadata checks, and stylometric analysis. It revealed the blind spots each tool leaves behind, similar to a smudged fingerprint on glass that never fully matches. These tools influence lessons, guide reporting, and generate online discussions. However, they still struggle with tampered metadata or a biased algorithm that distorts results like a smudge on glass. The paper calls for more research and careful ethical reflection on how AI changes authorship and academic integrity, similar to following the trail of ink to find out who really held the pen.
    Y shimmered softly, like a dark bead of ink catching a sliver of light. Cui and H. stood together, the faint scent of rain still clinging to their coats. They stood side by side while a light breeze brushed the edge of H.'s sleeve. In 2025, Zhang [11] looked at how well college students could recognize AIGC and decide if it seemed trustworthy-for example, when reading a short essay that an AI had typed out in crisp, even lines. When students in educational technology were surveyed, they recognized AIGC about 70% of the time, but their accuracy dropped to roughly 60% when the material became more specialized-such as a thick programming assignment filled with code snippets and jargon. Trust and real use depended on how genuinely welcome the technology felt, but AI's clever illusions can draw people in too deep-like relaxing to a calm voice that guesses instead of truly knowing. This study provides practical ways to improve AIGC literacy and encourage its responsible use across campus-such as reminding students to stop and check a source before they believe what's on the screen.
   She let out a soft gasp, just a quiet "Oh." The engine thrummed beneath the hood, low and steady, pulsing like a heartbeat you could feel through the metal. Talaver and T stood side by side, breathing in air that stung faintly with the sharp, metallic smell of rain about to fall. The single letter A gleamed on the page, sharp and pale like wet ink catching light on clean snow. In 2025, Vakaliuk built a layered system to verify AI-generated educational content, as careful as sorting colored papers under a bright desk lamp. The model identifies each factual claim, finds solid proof from reliable sources, and refines the material so it stays sharp and true for teaching, like drawing a bright line through tightly woven cloth. It combines NLP, evidence checks, and credibility scoring to help educators find reliable sources, marking each with sharp, annotated notes that pop like dark ink on a clean page. That's why this approach delivers steady, scalable results for AI-driven learning and content management, like effortlessly keeping thousands of student files sorted and easy to find.
   The letter D cut clean across the page, the ink glinting as if it hadn't quite dried along its perfect curve. In 2023, Weber-Wulff and her colleagues ran a thorough test of AI-generated text detection tools. They examined twelve public systems and two commercial ones, including Turnitin and PlagiarismCheck, comparing them line by line like detectives with magnifying glasses. In 2023, researchers carefully reviewed AI-generated text detectors, putting twelve public tools and two commercial ones, Turnitin and PlagiarismCheck, under a bright desk lamp to catch any flaws. The results showed that most tools still struggle to distinguish AI text from human writing. Many even incorrectly identify machine-generated paragraphs as genuine, confusing a chatbot's smooth reply with a quick note from a coworker. Additionally, using machine translation or masking tricks disrupts detection even further, like smearing a photo until the outlines fade into fog.
The soft smell of rain came through the open window, cool and fresh against the air. The letter A stood out sharp and clean, like fresh chalk on a board. In 2025, Najjar and his team built a tool to spot AI-generated content. They trained it on the CyberHumanAI dataset, which included 500 human texts and 500 pieces written by ChatGPT. Each sample was clear like fresh ink on a page. In 2025, they developed a tool to detect AI-generated writing, training it on the same CyberHumanAI dataset-500 human texts and 500 crafted by ChatGPT, where each line was as unique as ink compared to digital light. XGBoost and Random Forest achieved 83% and 81% accuracy, surpassing GPTZero by 48.5%. This difference is as clear as black ink versus a light pencil line. Using Explainable AI, their research revealed what makes human writing different from ChatGPT's. People took time for sensory moments, like the sound of a chair scraping or the smell of coffee. In contrast, the AI kept everything smooth and abstract.
 
III. METHODOLOGY
The study followed a clear, step-by-step process to evaluate how reliable and accurate AI-generated educational content was compared to human writing. We examined every result carefully, much like a teacher grading essays under a bright desk lamp. We created a large question bank covering Math, Science, History, Literature, and Geography, including quick warm-ups and challenging problems that made you pause, tapping your pencil on the desk. Skilled teachers prepared verified reference answers to set the standard, a clear yardstick against which every other response would be judged. We ran the same setup using several AI tools-ChatGPT, Gemini, Claude, and DeepSeek-displaying them side by side on a single laptop screen. After collecting and sorting all the replies, each one stacked like crisp sheets of paper on a desk, experts rated every AI response without knowing which was which to keep the assessment fair. We looked at whether the facts were correct, if the ideas were clear, how solid the reasoning was, and whether it truly helped someone learn, like seeing that quick spark in their eyes when it finally clicks. I rated each criterion from the list above, noting how well each one performed, like checking which note rings clear when you strike a chord. We grouped the errors into four types-factual, computational, outdated, and misleading-to find the common flaw that kept appearing, faint and stubborn like a fingerprint on clear glass. We lined up a handful of AI models to see how closely they reflected human answers, noting each score as if we were tallying on a whiteboard smudged with old ink. The results appear on a research dashboard called the AI Truth Meter, which tracks accuracy by subject, shows how mistakes spread through the data, and charts performance rising or falling over time. This approach gave us a clear, step-by-step way to assess how reliable AI tools really are for learning and academic work, much like examining each one under a lamp to spot tiny cracks. . Fig 3. Flowchart for spotting AI‑generated educational content, such as a lesson that sounds oddly polished and uniform.

IV. SYSTEM IMPLEMENTTION

   The project's web-based AI Truth Meter glows softly on the screen, pulsing as it evaluates each claim. It's built to assess how accurate and reliable AI-generated educational content can be, ensuring every detail aligns like neat lines from a bright green marker on a clean board. Researchers can pose their own specific questions and gather answers from ChatGPT, Gemini, Claude, and DeepSeek. Each response provides a unique flavor, similar to tasting four cups of the same tea, with each leaving hints of smoke or spice.
   The system collects every AI-generated reply, saves it, and compares it to expert reference answers, much like sliding a neat row of cards under a bright desk lamp to reveal small flaws. Human reviewers then evaluate the AI's performance by checking the accuracy of the facts, the strength of the logic, and the clarity of each answer. This process is akin to smoothing out a knot in a sentence until it flows well. The system computes an overall accuracy score and identifies areas where errors persist, including incorrect facts, math mistakes, or confusing ideas, similar to mixing up the heat setting with the number showing on the dial.They presented the results through an interactive dashboard that displays overall accuracy, performance by subject, and month-to-month progress. Slim color bars rise and fall across the screen like small pulses of light. This change makes it simpler to test, track, and understand how AI performs in classrooms. It's transparent and straightforward, allowing us to see how dependable its generated content really is for students, much like turning on a light when everything finally clicks into place.
V. RESULTS AND DISCUSSION
This study highlights both the potential and the limits of AI-made educational content. It resembles a bright bulb that hums and flickers at the edges. Using data from the AI Truth Meter platform, researchers conducted 2,847 tests across various academic subjects, with 1,234 users participating. Each user clicked through questions that appeared on their screens. AI tools show an average accuracy of about 82.4%, which indicates they usually produce solid content. However, they occasionally falter, appearing polished on the surface but stumbling over minor details, like a misspelled name.In terms of subject accuracy, Mathematics leads with 87%. Physics follows closely at 84%, while History and Literature lag behind, struggling with subtle context and shifting meanings that can turn a single phrase into a guessing game. Common mistakes included incorrect numbers, tangled ideas, and outdated facts. There were also gaps in context, like trying to complete a jigsaw puzzle with half the pieces missing. This shows that AI still struggles with deep reasoning and multi-step problems.Despite these flaws, accuracy increased by a modest 3.2% over time. This suggests that each new version is becoming more reliable, much like gears locking smoothly into place. Overall, the results indicate that AI tools are useful for quick explanations or for providing a boost in learning, but they still cannot match human understanding. This is particularly true when tasks require sharp reasoning, careful judgment, or a broader perspective that only people seem to grasp, like noticing the subtle shift of color before sunrise. In summary, AI-generated content should serve as a useful backup for learning, but it should always be reviewed by a person to ensure accuracy and reliability, like a second pair of eyes detecting a tiny smudge of ink on a page.


Fig  4.  Choose the AI model, paste in the text we need to check, and it quickly scans for accuracy-like watching a meter jump as data flows in.


Fig 5. Analysis results such as factual accuracy, logical consistency, clarity, and depth-like checking if each idea clicks together as cleanly as puzzle pieces..


Fig  6. Shows the problems discovered and the strengths that stand out, like a cracked step beside a solid stone wall.


Fig 7. Provides a clear, detailed breakdown of the statement, tracing each point like following ink lines across a page.


Fig 8. It displays the model's accuracy through clear graphs and colorful pie charts, each slice standing out like a small painted segment.
                                   
   
                                      VI.  CONCLUSION 
   This paper shows that while AI tools like ChatGPT, Gemini, Claude, and DeepSeek can create polished lessons, they still can't match the steady, human reliability you'd expect from a carefully written essay, the kind that leaves faint smudges of ink on the margin. According to the AI Truth Meter, most responses are quick, detailed, and smart. However, they occasionally stumble, leaving out facts, presenting shaky logic, or sharing data that feels outdated. These mistakes remind us that AI content works best as a helper; it serves as a guide for learning, not the final say. It's more like quick notes scrawled in the margin than the textbook next to it. With careful refinement and a steady human hand, AI tools can open up new paths to accessibility and customize learning for each student, like adjusting a text bar until every letter sharpens into view. Nevertheless, we need careful oversight and expert reviews to keep education accurate and trustworthy, similar to double-checking a fact before it makes it into the textbook. Ultimately, the project highlights how important it is to balance AI's quick precision with the steady judgment of real people so students can learn in an environment they can trust, year after year, like the steady hum of a familiar classroom.

VII. FUTURE WORK
   Down the road, we could grow this project in a few powerful directions, building better ways to judge AI-made lessons. For example, we could see how clearly a student understands a tough math idea after a quick demo. First, the comparison could go deeper by including more AI tools and covering a broader range of academic subjects. We could add a coding platform or two and a handful of research fields. The system can fact-check and flag plagiarism by itself. It catches mistakes faster than any person flipping through page after page of dense text. Another crucial step is collecting feedback from a mix of teachers and students on how clear and useful the AI's answers sound in real classes. For instance, when a teacher asks it to walk through a tough math problem written in chalk on the board. Adding machine learning that learns from past reviews and automatically judges how reliable new AI answers are would turn the platform into a real asset. It would be like giving it the sharp eye of a seasoned editor scanning a page under bright light. As we gather updates, the system gets sharper, more engaging, and far more flexible. It helps students, teachers, and researchers see AI's strengths and limits in real time, like when it spots a faint logic slip in a rough essay draft.

VIII. REFERENCES

[1] B. Sharimbayev and S. Kadyrov, "Text Classification for AI Generated Content with Machine Learning and Deep Learning Models," 2025 IEEE 5th International Conference on Smart Information Systems and Technologies (SIST), Astana, Kazakhstan, 2025, pp. 1-5, doi: 10.1109/SIST61657.2025.11139329.
[2] Erol, G., Ergen, A., Gülşen Erol, B., Kaya Ergen, Ş., Bora, T. S., Çölgeçen, A. D., ... & Güngör, A. (2025). Can we trust academic AI detective? Accuracy and limitations of AI-output detectors. Acta neurochirurgica, 167(1), 1-12.
[3] C. R. G. R. Reddy, D. S. Muthukumar, T. R. B, N. Ganesh,M. Rafi and P. Vamsi, "AI vs Human Text Detector: Identifying AI-Generated Content Using NLP," 2025 3rd International Conference on Inventive Computing and Informatics (ICICI), Bangalore, India, 2025,pp.126132,doi:10.1109/ICICI65870.2025.11069552.
[4] M. R. Shoaib, Z. Wang, M. T. Ahvanooey and J. Zhao, "Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models," 2023 International Conference on Computer and Applications (ICCA), Cairo, Egypt, 2023, pp. 1-7, doi: 10.1109/ICCA59364.2023.10401723.
[5] L. Liu, D. Zhang, B. Yan and D. Wu, "EduGuard-LLM: An AI-Generated Content Detector Using Large Language Models for Safeguarding Educational Integrity," 2024 4th International Conference on Educational Technology (ICET), Wuhan, China, 2024, pp. 102-105, doi: 10.1109/ICET62460.2024.10869067.
[6] M. P. Kalra, A. Mathur and C. Patvardhan, "Detection of AI-generated Text: An Experimental Study," 2024 IEEE 3rd World Conference on Applied Intelligence and Computing (AIC), Gwalior, India, 2024, pp. 552-557, doi: 10.1109/AIC61668.2024.10731116.
[7] R. Raja Subramanian, B. Surekha, P. SubbaRao, S. M and P. D. Sai Chakravarthy, "Automating the detection of plagiarism in educational content using Natural language processing," 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE), Virudhunagar, India, 2025, pp. 1-6, doi: 10.1109/ICCRTEE64519.2025.11052934.
[8] C. Mohammed and S. Rocke, "WIP Post-Assessment Processes Given the Rise of Generative AI: Findings from the Literature," 2024 IEEE Frontiers in Education Conference (FIE), Washington, DC, USA, 2024, pp. 1-4, doi: 10.1109/FIE61694.2024.10893183.
[9] Elkhatat, A. M., Elsaid, K., & Almeer, S. (2023). Evaluating the efficacy of AI content detection tools in differentiating between human and AI-generated text. International Journal for Educational Integrity, 19(1), 1-16.
[10] Uzun, L. (2023). ChatGPT and academic integrity concerns: Detecting artificial intelligence generated content. Language Education and Technology, 3(1).
[11] Cui, Y., & Zhang, H. (2025). Can student accurately identify artificial intelligence generated content? an exploration of AIGC credibility from user perspective in education. Education and Information Technologies, 1-26.
[12] Talaver, O. V., & Vakaliuk, T. A. (2025). A model for improving the accuracy of educational content created by generative AI. In CEUR Workshop Proceedings (pp. 149-158).
[13] Weber-Wulff, D., Anohina-Naumeca, A., Bjelobaba, S., Foltýnek, T., Guerrero-Dib, J., Popoola, O., ... & Waddington, L. (2023). Testing of detection tools for AI-generated text. International Journal for Educational Integrity, 19(1), 1-39.
[14] Najjar, A. A., Ashqar, H. I., Darwish, O. A., & Hammad, E. (2025). Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity. arXiv preprint arXiv:2501.03203.
[15] Pan, W. H., Chok, M. J., Wong, J. L. S., Shin, Y. X., Poon, Y. S., Yang, Z., ... & Lim, M. K. (2024, April). Assessing ai detectors in identifying ai-generated code: Implications for education. In Proceedings of the 46th international conference on software engineering: software engineering education and training (pp. 1-11).






